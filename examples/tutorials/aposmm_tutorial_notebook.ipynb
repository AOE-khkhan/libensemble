{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Optimization with APOSMM\n",
    "\n",
    "This tutorial demonstrates libEnsemble’s capability to identify multiple minima of simulation output using the built-in APOSMM (Asynchronously Parallel Optimization Solver for finding Multiple Minima) ``gen_f``. In this tutorial, we’ll create a simple simulation ``sim_f`` that defines a function with multiple minima, then write a libEnsemble calling script that imports APOSMM and parameterizes it to check for minima over a domain of outputs from our ``sim_f``.\n",
    "\n",
    "## Six-Hump Camel Simulation Function\n",
    "\n",
    "Describing APOSMM’s operations is simpler with a given function on which to depict evaluations. We’ll use the Six-Hump Camel function, known to have six global minima. A sample space of this function, containing all minima, appears below:\n",
    "\n",
    "![6humpcamel](images/basic_6hc.png)\n",
    "\n",
    "*Note: The following ``sim_f`` won't operate stand-alone since it has not yet been parameterized and called by libEnsemble. The full routine should work as expected.*\n",
    "\n",
    "Create a new Python file named ``six_hump_camel.py``. This will be our ``sim_f``, incorporating the above function. Write the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def six_hump_camel(H, persis_info, sim_specs, _):\n",
    "    \"\"\"Six-Hump Camel sim_f.\"\"\"\n",
    "\n",
    "    batch = len(H['x'])                            # Num evaluations each sim_f call.\n",
    "    H_o = np.zeros(batch, dtype=sim_specs['out'])  # Define output array H\n",
    "\n",
    "    for i, x in enumerate(H['x']):\n",
    "        H_o['f'][i] = three_hump_camel_func(x)     # Function evaluations placed into H\n",
    "\n",
    "    return H_o, persis_info\n",
    "\n",
    "\n",
    "def six_hump_camel_func(x):\n",
    "    \"\"\" Six-Hump Camel function definition \"\"\"\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    term1 = (4-2.1*x1**2+(x1**4)/3) * x1**2\n",
    "    term2 = x1*x2\n",
    "    term3 = (-4+4*x2**2) * x2**2\n",
    "\n",
    "    return term1 + term2 + term3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APOSMM Operations\n",
    "\n",
    "APOSMM coordinates multiple local optimization runs starting from a collection of sample points. These local optimization runs occur in parallel, and can incorporate a variety of optimization methods, including from NLopt, PETSc/TAO, and SciPy. Some number of uniformly sampled points is returned by APOSMM for simulation evaluations before local optimization runs can occur, if no prior simulation evaluations are provided. User-requested sample points can also be provided to APOSMM:\n",
    "\n",
    "![6hcsampling](images/sampling_6hc.png)\n",
    "\n",
    "Specifically, APOSMM will begin local optimization runs from those points that don’t have better (more minimal) points nearby within a threshold ``r_k``. For the above example, after APOSMM has returned the uniformly sampled points, for simulation evaluations it will likely begin local optimization runs from the user-requested approximate minima. Providing these isn’t required, but can offer performance benefits.\n",
    "\n",
    "Each local optimization run chooses new points and determines if they’re better by passing them back to be evaluated by the simulation routine. If so, new local optimization runs are started from those points. This continues until each run converges to a minimum:\n",
    "\n",
    "![6hclocalopt](images/localopt_6hc.png)\n",
    "\n",
    "Throughout, generated and evaluated points are appended to the History array, with the field ``'local_pt'`` being ``True`` if the point is part of a local optimization run, and ``'local_min'`` being ``True`` if the point has been ruled a local minimum.\n",
    "\n",
    "## APOSMM Persistence\n",
    "\n",
    "The most recent version of APOSMM included with libEnsemble is referred to as Persistent APOSMM. Unlike most other user functions that are initiated and completed by workers multiple times based on allocation, a single worker process initiates APOSMM so that it “persists” and keeps running over the course of the entire libEnsemble routine. APOSMM begins it’s own parallel evaluations and communicates points back and forth with the manager, which are then given to workers and evaluated by simulation routines.\n",
    "\n",
    "In practice, since a single worker becomes “persistent” for APOSMM, users must ensure that enough workers or MPI ranks are initiated to support libEnsemble’s manager, a persistent worker to run APOSMM, and simulation routines. The following:\n",
    "\n",
    "    mpiexec -n 3 python my_aposmm_routine.py\n",
    "    \n",
    "results in only one worker process available to perform simulation routines.\n",
    "\n",
    "## Calling Script\n",
    "\n",
    "Create a new Python file named ``my_first_aposmm.py``. Start by importing NumPy, libEnsemble routines, APOSMM, our ``sim_f``, and a specialized allocation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'six_hump_camel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-598930408e78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msix_hump_camel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix_hump_camel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlibensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibE\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'six_hump_camel'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from six_hump_camel import six_hump_camel\n",
    "\n",
    "from libensemble.libE import libE\n",
    "from libensemble.gen_funcs.persistent_aposmm import aposmm\n",
    "from libensemble.alloc_funcs.persistent_aposmm_alloc import persistent_aposmm_alloc\n",
    "from libensemble.tools import parse_args, add_unique_random_streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allocation function starts a single Persistent APOSMM routine and provides ``sim_f`` output for points requested by APOSMM. Points can be sampled points or points from local optimization runs.\n",
    "\n",
    "APOSMM supports a wide variety of external optimizers. The following statements set optimizer settings to ``'scipy'`` to indicate to APOSMM which optimization method to use, and help prevent unnecessary imports or package installations:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
